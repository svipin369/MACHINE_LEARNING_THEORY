{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment_18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1.What is the difference between supervised and unsupervised learning? Give some examples to\n",
    "illustrate your point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 1\n",
    "In a supervised learning model, the algorithm learns on a labeled dataset, providing an answer key that \n",
    "the algorithm can use to evaluate its accuracy on training data. An unsupervised model, in contrast, \n",
    "provides unlabeled data that the algorithm tries to make sense of by extracting features and patterns on\n",
    "its own.\n",
    "Unsupervised ML Examples\n",
    "1.A subgroup of cancer patients grouped by their gene expression measurements\n",
    "2.Groups of shopper based on their browsing and purchasing histories\n",
    "3.Movie group by the rating given by movies viewers\n",
    "\n",
    "Supervised ML examples\n",
    "1.Identify whether it is cat or dog\n",
    "2.House price prediction\n",
    "3.Predicting weather conditions\n",
    "Hence input data is labeled here whereas in unsupervised the input data is not labeled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2. Mention a few unsupervised learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 2\n",
    "1.A subgroup of cancer patients grouped by their gene expression measurements\n",
    "2.Groups of shopper based on their browsing and purchasing histories\n",
    "3.Movie group by the rating given by movies viewers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3. What are the three main types of clustering methods? Briefly describe the characteristics of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 3\n",
    "Clustering methods are a category of unsupervised machine learning techniques used to group similar data points into clusters. The three main types of clustering methods are:\n",
    "\n",
    "Hierarchical Clustering:\n",
    "\n",
    "Characteristics: Hierarchical clustering creates a tree-like structure of clusters, known as a dendrogram, where data points are grouped based on their similarity. It can be agglomerative (bottom-up) or divisive (top-down).\n",
    "Agglomerative: Starts with individual data points as clusters and merges them step by step into larger clusters based on similarity.\n",
    "Divisive: Begins with all data points in one cluster and recursively splits them into smaller clusters.\n",
    "No need to specify the number of clusters in advance.\n",
    "Provides a visual representation of cluster hierarchy.\n",
    "\n",
    "K-Means Clustering:\n",
    "\n",
    "Characteristics: K-Means is a partitioning method that aims to divide data into a pre-specified number (K) of clusters. It assigns data points to the nearest cluster center based on their distance (typically Euclidean) to the center.\n",
    "Requires the user to specify the number of clusters (K) in advance.\n",
    "Often computationally efficient and works well with large datasets.\n",
    "Sensitive to the initial placement of cluster centers, and it may converge to local optima.\n",
    "\n",
    "Density-Based Clustering (DBSCAN - Density-Based Spatial Clustering of Applications with Noise):\n",
    "\n",
    "Characteristics: DBSCAN identifies clusters based on the density of data points. It defines clusters as areas of high data point density separated by areas of lower density.\n",
    "Does not require specifying the number of clusters in advance.\n",
    "Can discover clusters of arbitrary shapes and is robust to noise.\n",
    "Suitable for data with non-uniform density and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4. Explain how the k-means algorithm determines the consistency of clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 4\n",
    "1.K centroids are created randomly (based on the predefined value of K)\n",
    "2.K-means allocates every data point in the dataset to the nearest centroid (minimizing Euclidean distances\n",
    "between them), meaning that a data point is considered to be in a particular cluster if it is closer to\n",
    "that cluster’s centroid than any other centroid.\n",
    "3.Then K-means recalculates the centroids by taking the mean of all data points assigned to that centroid’s\n",
    "cluster, hence reducing the total intra-cluster variance in relation to the previous step. The “means” in \n",
    "the K-means refers to averaging the data and finding the new centroid.\n",
    "4.The algorithm iterates between steps 2 and 3 until some criteria is met (e.g. the sum of distances between \n",
    "the data points and their corresponding centroid is minimized, a maximum number of iterations is reached, \n",
    "no changes in centroids value or no data points change clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5. With a simple illustration, explain the key difference between the k-means and k-medoids\n",
    "algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 5\n",
    "K-Means and K-Medoids are both clustering algorithms, but they differ in how they define and update cluster centers. Here's a simple illustration to explain the key difference between the two:\n",
    "\n",
    "K-Means:\n",
    "In the K-Means algorithm, cluster centers are represented by the mean (average) of data points in each cluster. The algorithm iteratively updates these means to minimize the sum of squared distances from data points to their respective cluster center. This process continues until convergence.\n",
    "\n",
    "a)Initialize cluster centers (centroids) randomly.\n",
    "b)Assign each data point to the nearest cluster based on distance (usually Euclidean).\n",
    "c)Recalculate the cluster centers as the mean of all data points in the cluster.\n",
    "Repeat steps 2 and 3 until convergence (minimal change in cluster assignments or centroids).\n",
    "\n",
    "Illustration:\n",
    "Imagine you have a dataset with data points (dots) and you want to find two clusters (K=2). K-Means will calculate the mean of the data points in each cluster and move the cluster centers to these means, trying to minimize the sum of squared distances from each point to its cluster center.\n",
    "\n",
    "\n",
    "K-Medoids:\n",
    "In contrast, K-Medoids uses the medoid, which is the most centrally located data point in a cluster, as the representative cluster center. The algorithm aims to minimize the sum of distances from data points to the medoid. Like K-Means, it iteratively updates these medoids until convergence.\n",
    "\n",
    "a)Initialize cluster medoids (data points) randomly.\n",
    "b)Assign each data point to the nearest cluster based on distance (e.g., Euclidean).\n",
    "c)Recalculate the cluster medoids as the data point that minimizes the total distance to other points in the cluster.\n",
    "Repeat steps 2 and 3 until convergence (minimal change in cluster assignments or medoids).\n",
    "Illustration:\n",
    "For the same dataset, K-Medoids identifies the most central data point in each cluster and uses that point as the cluster center (medoid), which may not necessarily be the mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6. What is a dendrogram, and how does it work? Explain how to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 6\n",
    "A dendrogram is a tree-like diagram used in hierarchical clustering to visualize the arrangement of data points into clusters based on their similarity. It provides a hierarchical representation of the clustering process, showing how data points are grouped together at various levels of granularity. Dendrograms are especially useful for understanding the structure and relationships within the data.\n",
    "\n",
    "Here's how a dendrogram works and how to create one:\n",
    "\n",
    "a)Distance Matrix: To create a dendrogram, you start with a distance matrix that represents the pairwise distances between all data points. The distance can be measured using various distance metrics, such as Euclidean distance, Manhattan distance, or others, depending on the nature of the data and the problem.\n",
    "\n",
    "b)Agglomerative Clustering: Dendrograms are typically used in agglomerative hierarchical clustering, which is a bottom-up approach. Initially, each data point is considered a single cluster.\n",
    "\n",
    "c)Merging Clusters: The algorithm proceeds by iteratively merging the two closest clusters, creating a new, larger cluster at each step. The choice of a distance metric for merging clusters can vary (e.g., single linkage, complete linkage, average linkage), affecting the shape of the dendrogram.\n",
    "\n",
    "c)Dendrogram Structure: The dendrogram is constructed by visualizing the process of merging clusters. At each step, you create a branch in the dendrogram that represents the merger of clusters. The height or length of each branch represents the distance or dissimilarity between the merged clusters. Shorter branches indicate closely related clusters, while longer branches indicate more dissimilar clusters.\n",
    "\n",
    "d)Termination: The process continues until all data points are in a single cluster at the root of the dendrogram. The dendrogram effectively shows the hierarchy of cluster formations, and you can choose to cut the dendrogram at a specific height or depth to obtain a certain number of clusters.\n",
    "\n",
    "To create a dendrogram, you can use various programming libraries or software tools designed for hierarchical clustering. Here's a simplified example using Python and the SciPy library:\n",
    "\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "data = [[1, 2], [2, 3], [5, 6], [8, 9]]\n",
    "distance_matrix = sch.distance.pdist(data)\n",
    "\n",
    "\n",
    "linkage_matrix = sch.linkage(distance_matrix, method='single')\n",
    "\n",
    "dendrogram = sch.dendrogram(linkage_matrix)\n",
    "plt.show()\n",
    "\n",
    "This code creates a dendrogram for a small dataset, but you can adapt it to your specific data and distance metrics. The resulting dendrogram helps you understand the hierarchical structure of clusters and make decisions about how to cut the tree to obtain the desired number of clusters for your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7. What exactly is SSE? What role does it play in the k-means algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 7\n",
    "\n",
    "SSE stands for \"Sum of Squared Errors,\" and it is a metric used to evaluate the performance of clustering algorithms, including the K-Means algorithm. SSE is also sometimes referred to as \"Inertia\" in the context of K-Means.\n",
    "\n",
    "SSE measures the compactness or cohesion of clusters in a K-Means clustering. It is calculated as the sum of the squared Euclidean distances between each data point and its assigned cluster center (centroid). In other words, SSE quantifies how close data points are to the centroid of their respective clusters. The formula for calculating SSE is as follows:\n",
    "\n",
    "SSE = Σ (distance(point_i, centroid_i)²)\n",
    "\n",
    "Where:\n",
    "\n",
    "point_i is a data point.\n",
    "centroid_i is the centroid of the cluster to which point_i belongs.\n",
    "distance(point_i, centroid_i) is the Euclidean distance between the data point and its assigned centroid.\n",
    "The role of SSE in the K-Means algorithm is twofold:\n",
    "\n",
    "Clustering Quality Evaluation: SSE serves as an internal evaluation metric for K-Means. The algorithm aims to minimize SSE during its iterative optimization process. It does so by adjusting the cluster centroids and reassigning data points to clusters to reduce the total squared distances between data points and their cluster centers. Lower SSE values indicate better cluster quality, as it suggests that the data points are closer to their respective centroids.\n",
    "\n",
    "Determination of the Optimal Number of Clusters: SSE can also be used to help determine the optimal number of clusters (K) in K-Means. Typically, you would run K-Means with different values of K and calculate the SSE for each K. As K increases, the SSE tends to decrease because smaller clusters result in data points being closer to their centroids. However, at a certain point, further increasing K may result in diminishing returns, and the SSE may level off. The \"elbow method\" is a common technique to select an appropriate K by visually inspecting the SSE curve and looking for an \"elbow\" point where the rate of improvement in SSE starts to slow down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8. With a step-by-step algorithm, explain the k-means procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 8\n",
    "Let  X = {x1,x2,x3,……..,xn} be the set of data points and V = {v1,v2,…….,vc} be the set of centers.\n",
    "\n",
    "1) Randomly select ‘c’ cluster centers.\n",
    "\n",
    "2) Calculate the distance between each data point and cluster centers.\n",
    "\n",
    "3) Assign the data point to the cluster center whose distance from the cluster center is minimum of all the\n",
    "   cluster centers..\n",
    "\n",
    "4) Recalculate the new cluster center using: \n",
    "\n",
    "where, ‘ci’ represents the number of data points in ith cluster.\n",
    "\n",
    "\n",
    "5) Recalculate the distance between each data point and new obtained cluster centers.\n",
    "\n",
    "6) If no data point was reassigned then stop, otherwise repeat from step 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 9. In the sense of hierarchical clustering, define the terms single link and complete link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 9\n",
    "In single-link clustering or single-linkage clustering , the similarity of two clusters is the similarity\n",
    "of their most similar members . This single-link merge criterion is local. We pay attention solely to the\n",
    "area where the two clusters come closest to each other. Other, more distant parts of the cluster and the \n",
    "clusters' overall structure are not taken into account.\n",
    "\n",
    "In complete-link clustering or complete-linkage clustering , the similarity of two clusters is the \n",
    "similarity of their most dissimilar members. This is equivalent to choosing the \n",
    "cluster pair whose merge has the smallest diameter. This complete-link merge criterion is non-local; the \n",
    "entire structure of the clustering can influence merge decisions. This results in a preference for compact\n",
    "clusters with small diameters over long, straggly clusters, but also causes sensitivity to outliers. A\n",
    "single document far from the center can increase diameters of candidate merge clusters dramatically and\n",
    "completely change the final clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 10. How does the apriori concept aid in the reduction of measurement overhead in a business\n",
    "basket analysis? Give an example to demonstrate your point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 10\n",
    "The Apriori concept is a key technique in association rule mining, a data mining method used for analyzing patterns in large datasets, particularly in the context of market basket analysis. Apriori helps in the reduction of measurement overhead in business basket analysis by identifying and focusing on the most relevant and frequent itemsets, thereby reducing the number of itemsets to be examined. This concept uses the \"apriori property,\" which implies that if an itemset is infrequent, then all of its supersets (containing more items) are also infrequent. By leveraging this property, Apriori prunes infrequent itemsets, which can significantly reduce computational overhead.\n",
    "\n",
    "Here's an example to illustrate how Apriori aids in the reduction of measurement overhead in business basket analysis:\n",
    "\n",
    "Scenario:\n",
    "\n",
    "Imagine you are the manager of a grocery store, and you want to understand the purchasing patterns of your customers to optimize product placement and promotions. You have sales transaction data, and you want to identify associations between items that are frequently bought together.\n",
    "\n",
    "Without Apriori:\n",
    "If you were to examine all possible combinations of items in customer transactions without using the Apriori concept, you would face a combinatorial explosion. For instance, if you have 100 different products, there are 2^100 (over a nonillion) possible itemsets, which is computationally infeasible to analyze.\n",
    "\n",
    "With Apriori:\n",
    "Apriori helps you focus on the most frequent itemsets, making the analysis more manageable. Here's how it works:\n",
    "\n",
    "Frequency Threshold: You set a minimum support threshold. This threshold represents the minimum occurrence (or frequency) of an itemset in the dataset to be considered significant. For example, you might set a threshold of 1% (0.01) to identify itemsets that occur in at least 1% of transactions.\n",
    "\n",
    "Candidate Generation: Apriori starts by identifying all the individual items that meet the support threshold (called \"frequent 1-itemsets\"). In our grocery store example, these could be items like \"bread,\" \"milk,\" \"eggs,\" etc.\n",
    "\n",
    "Iterative Approach: Apriori then iteratively generates candidate itemsets with more items, pruning those that do not meet the minimum support threshold. It uses the apriori property, meaning that if an itemset is infrequent, all its supersets are also infrequent.\n",
    "\n",
    "Associations: The remaining frequent itemsets represent associations between products that are frequently purchased together. These associations can be used to optimize product placement, cross-selling, and marketing strategies.\n",
    "\n",
    "Example:\n",
    "\n",
    "Suppose you find that the frequent 2-itemset {bread, milk} occurs in 3% of transactions, and the frequent 2-itemset {bread, eggs} occurs in 2% of transactions. These associations suggest that when customers buy \"bread,\" they are likely to buy \"milk\" or \"eggs\" as well.\n",
    "\n",
    "By using Apriori, you've reduced the measurement overhead by focusing on relevant itemsets and associations, making it practical to analyze and act on the insights gained from market basket analysis. This, in turn, can lead to improved sales and customer satisfaction in your grocery store."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
