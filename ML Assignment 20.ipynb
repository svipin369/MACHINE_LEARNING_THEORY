{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment_20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1. What is the underlying concept of Support Vector Machines?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 1\n",
    "Support Vector Machines (SVMs) are a type of supervised machine learning algorithm used for classification and regression tasks. The underlying concept of SVMs is to find a hyperplane that best separates data points belonging to different classes in a high-dimensional space. The key ideas behind SVMs include:\n",
    "\n",
    "a)Maximizing Margin: SVM aims to find a hyperplane that maximizes the margin between the classes. The margin is defined as the distance between the hyperplane and the nearest data points (support vectors) from each class. Maximizing this margin helps improve the model's generalization and robustness.\n",
    "\n",
    "b)Linear Separability: SVMs work well when the data is linearly separable, meaning it can be cleanly divided into two classes by a straight line or hyperplane. However, SVMs can also be used with non-linear data by employing techniques like the kernel trick, which maps the data into a higher-dimensional space where it may become linearly separable.\n",
    "\n",
    "c)Support Vectors: Support vectors are the data points that are closest to the hyperplane and have the smallest margin. These points are crucial in defining the hyperplane and the margin. The positions of support vectors determine the final decision boundary.\n",
    "\n",
    "c)Kernel Trick: SVMs can handle non-linear data by transforming it into a higher-dimensional space through a kernel function. This transformation allows for non-linear decision boundaries in the original feature space.\n",
    "\n",
    "d)Regularization: SVMs incorporate regularization to control overfitting. The regularization parameter (C) balances the trade-off between maximizing the margin and minimizing classification errors. Smaller values of C prioritize a wider margin, potentially allowing some misclassifications, while larger values of C lead to a narrower margin and fewer misclassifications.\n",
    "\n",
    "e)Classification: In the case of classification, SVMs assign data points to one of two classes based on which side of the hyperplane they fall. For multi-class problems, SVMs can be extended using techniques like one-vs-all (OvA) or one-vs-one (OvO) to handle multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2. What is the concept of a support vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 2\n",
    "Support vectors are data points that are closer to the hyperplane and influence the position and \n",
    "orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. \n",
    "Deleting the support vectors will change the position of the hyperplane. These are the points that help us\n",
    "build our SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3. When using SVMs, why is it necessary to scale the inputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 3\n",
    "Because Support Vector Machine (SVM) optimization occurs by minimizing the decision vector w, the optimal\n",
    "hyperplane is influenced by the scale of the input features and it’s therefore recommended that data be \n",
    "standardized (mean 0, var 1) prior to SVM model training.\n",
    "Advantage of scaling is to avoid attributes in greater numeric ranges dominating those in smaller \n",
    "numeric ranges. Another advantage is to avoid numerical difficulties during the kernel calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4. When an SVM classifier classifies a case, can it output a confidence score? What about a\n",
    "percentage chance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 4\n",
    "Yes, an SVM classifier can provide a confidence score, but it doesn't directly give a percentage chance or probability. SVMs are primarily designed for binary classification, where they determine whether a data point belongs to one of the two classes. However, in certain cases, you can obtain a confidence score or a measure of how far a data point is from the decision boundary.\n",
    "\n",
    "Here's how you can get a confidence score or distance measure from an SVM:\n",
    "\n",
    "Distance from Hyperplane: You can compute the distance between a data point and the decision boundary (the hyperplane). This distance can be used as a confidence score. In SVM, this distance is often called the \"margin.\" Data points closer to the hyperplane have a lower margin and are more confident predictions, while those farther from the hyperplane have a higher margin and may be considered less confident predictions.\n",
    "\n",
    "Signed Margin: The sign of the distance from the hyperplane can be used to determine the class label. If the distance is positive, the data point is on one side of the decision boundary, and if it's negative, it's on the other side. The magnitude of this distance can be used as a confidence measure.\n",
    "\n",
    "Decision Function Output: In some SVM implementations, the decision function may provide a continuous output that is not directly a probability but can be used as a confidence score. This output is often used to rank data points by their confidence in belonging to a particular class.\n",
    "\n",
    "However, it's important to note that SVMs do not naturally output probabilities or percentage chances like some other classifiers (e.g., logistic regression or Naive Bayes). If you need probability estimates, you can use methods like Platt scaling or logistic regression calibration to convert the SVM's output into a probability score. These methods fit a logistic regression model to the SVM's decision function output to estimate class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5. Should you train a model on a training set with millions of instances and hundreds of features\n",
    "using the primal or dual form of the SVM problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 5\n",
    "This question applies only to linear SVMs since kernelized can only use the dual form. The computational complexity of the primal form of the SVM problem is proportional to the number of training instances m, while the computational complexity of the dual form is proportional to a number between m2 and m3. So if there are millions of instances, you should definitely use the primal form, because the dual form will be much too slow. The dual problem is faster to solve than the primal when the number of training instances is smaller than the number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6. Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the\n",
    "training collection. Is it better to raise or lower (gamma)? What about the letter C?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 6\n",
    "There might be too much regularization. To decrease it, you need to increase gamma or C (or both). Increasing gamma makes the bell-shape curve narrower, and as a result each instance's range of influence is smaller: the decision boundary ends up being more irregular, wiggling around individual instances. Conversely, a small gamma value makes the bell-shaped curve wider, so instances have a larger range of influence, and the decision boundary ends up smoother. A smaller C value leads to a wider street but more margin violations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Question 7\n",
    "To solve the soft margin linear Support Vector Machine (SVM) classifier problem using an off-the-shelf Quadratic Programming (QP) solver, you need to set up the QP problem with appropriate parameters (H, f, A, and b). Here's how these parameters should be set for the soft margin linear SVM:\n",
    " \n",
    "Answer 7\n",
    "Objective Function (H and f):\n",
    "\n",
    "H (Hessian matrix): This is a matrix that depends on your specific SVM formulation. For a soft margin linear SVM, H is often a matrix where its diagonal elements are 1/(2*C), and all other elements are 0, where C is the regularization parameter. The regularization parameter controls the trade-off between maximizing the margin and minimizing the classification error. The larger C, the more emphasis on classification accuracy.\n",
    "f: This is a vector representing the linear part of the objective function. In the case of the soft margin linear SVM, it's often a vector of zeros since there is no linear term in the objective function.\n",
    "Inequality Constraints (A and b):\n",
    "\n",
    "A: The matrix A is determined by the training data. Each row of A corresponds to a data point, and each column corresponds to a feature. The elements of A are the products of the class labels (1 or -1 for binary classification) and the feature values.\n",
    "b: The vector b is also determined by the training data and class labels. It's a column vector with elements of 1 for each data point in the positive class and -1 for each data point in the negative class. In the case of soft margin SVM, you can add additional constraints to ensure that data points lie outside of the margin, such as -1 for all data points to the left of the margin and 0 for data points inside the margin.\n",
    "Equality Constraints (if needed):\n",
    "\n",
    "In the soft margin SVM formulation, you typically don't have equality constraints. However, if you have specific constraints on your data, you can add them as equality constraints.\n",
    "Once you've set up the QP problem with these parameters, you can use an off-the-shelf QP solver to find the solution that defines the optimal hyperplane and support vectors for your soft margin SVM. The solver will give you the coefficients of the hyperplane and support vector weights, which can be used for classification.7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should\n",
    "the QP parameters (H, f, A, and b) be set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and\n",
    "an SGDClassifier. See if you can get them to make a model that is similar to yours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 8\n",
    "\n",
    "Of course it depends on the dataset and of course a lot of other factors add weight but today in this small post I’ll demonstrate how to use LinearSVC , SVC classifier and SGD classifier via Python code and also compare results for the same dataset.\n",
    "Since classifiers are very sensitive to outliers we need to scale them but before we need to pick the right dataset, for simpler results I’ll showcase the benchmark dataset for all classifiers — the Iris dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 9. On the MNIST dataset, train an SVM classifier. You&#39;ll need to use one-versus-the-rest to assign all\n",
    "10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want\n",
    "to tune the hyperparameters using small validation sets. What level of precision can you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer 9\n",
    "\n",
    "Training a Support Vector Machine (SVM) classifier on the MNIST dataset for digit recognition is a common and well-studied task. \n",
    "The MNIST dataset consists of 28x28 pixel grayscale images of handwritten digits from 0 to 9. Since SVM classifiers are binary, \n",
    "you would typically use a one-vs-the-rest (OvR) strategy to classify all 10 digits. Here's a high-level outline of the steps you can follow:\n",
    "\n",
    "a)Data Preprocessing:\n",
    "\n",
    "Load the MNIST dataset and preprocess the data. You'll need to flatten the 28x28 images into a 1D array and scale the pixel values to the range [0, 1].\n",
    "\n",
    "b)Train-Validation-Test Split:\n",
    "\n",
    "Split the dataset into three parts: a training set, a validation set, and a test set. \n",
    "The training set is used to train the model, the validation set helps you tune hyperparameters,\n",
    "and the test set is used to evaluate the model's performance.\n",
    "\n",
    "c)Feature Scaling:\n",
    "\n",
    "Standardize or normalize the feature values if needed.\n",
    "\n",
    "d)SVM Training:\n",
    "\n",
    "Train 10 SVM classifiers, one for each digit, using the OvR strategy. You can use an off-the-shelf SVM implementation,\n",
    "such as SVM from Scikit-Learn in Python. Tune hyperparameters (e.g., the regularization parameter C and choice of kernel) using the validation set to find the best settings for your model.\n",
    "\n",
    "d)Evaluation:\n",
    "\n",
    "Evaluate the SVM classifier using the test set. Common performance metrics for classification tasks include accuracy, precision, recall, F1-score, and confusion matrix. \n",
    "These metrics will give you an idea of the model's performance.\n",
    "\n",
    "Fine-Tuning and Optimization:\n",
    "\n",
    "If your initial model doesn't achieve the desired level of precision, you can further optimize it\n",
    "by exploring different kernels (e.g., linear, polynomial, or radial basis function), tuning the regularization\n",
    "parameter, and considering techniques like cross-validation for hyperparameter selection.\n",
    "The level of precision you can achieve will depend on several factors, including the choice of \n",
    "hyperparameters, the SVM kernel, and the quality of your training data.\n",
    "Achieving high precision on the MNIST dataset is certainly feasible. You can aim for precision values well above 95%, \n",
    "and with careful tuning and feature engineering, you might even approach or exceed 98% precision. However, achieving extremely\n",
    "high precision may require more complex models, such as convolutional neural networks (CNNs), which have proven to\n",
    "be highly effective on image classification tasks like MNIST."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
