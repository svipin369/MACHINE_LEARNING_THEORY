{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "national-electron",
   "metadata": {},
   "source": [
    "# Assignment_14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-glory",
   "metadata": {},
   "source": [
    "Question 1. What is the concept of supervised learning? What is the significance of the name?\n",
    "\n",
    "Answer 1)\n",
    "\n",
    "Supervised learning is a fundamental concept in machine learning, and its name carries a significant meaning.\n",
    "\n",
    "Imagine you're teaching a child how to identify different animals. You show the child pictures of animals and tell them the names of each animal. For example, you show a picture of a dog and say, \"This is a dog.\" Then, you show a picture of a cat and say, \"This is a cat.\" You continue this process for various animals.\n",
    "\n",
    "In supervised learning, the computer learns in a similar way, but instead of animals, it learns to make predictions or decisions based on input data. Here's how it works:\n",
    "\n",
    "Supervision: Just like you provided supervision to the child by telling them the correct names of animals, in supervised learning, the computer is provided with a labeled dataset. This dataset consists of input data and corresponding correct answers or labels.\n",
    "\n",
    "Learning: The computer's goal is to learn a mapping or relationship between the input data and the correct answers. It learns from the examples in the dataset to make predictions or decisions on new, unseen data.\n",
    "\n",
    "Generalization: Once the computer has learned from the labeled data, it can apply that knowledge to new, unseen data to make predictions or classifications. This is similar to the child being able to identify new animals they've never seen before based on their previous learning.\n",
    "\n",
    "The significance of the name \"supervised learning\" comes from the idea that the learning process is supervised or guided by the labeled examples in the training data. It's like a teacher supervising a student's learning by providing correct answers initially, allowing the student (in this case, the computer) to learn and generalize from those examples.\n",
    "\n",
    "In simple words, supervised learning is like teaching a computer to make decisions or predictions by showing it examples with correct answers and letting it learn from those examples so that it can apply that knowledge to new situations. It's a bit like training a digital apprentice with a teacher's guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-egyptian",
   "metadata": {},
   "source": [
    "Question 2. In the hospital sector, offer an example of supervised learning.\n",
    "\n",
    "Answer 2)\n",
    "Several surgeries and therapies like chemotherapy are last resort method to patients debilitating them. These surgeries often run a risk of death if a combination of parameters like oxygen saturation level are not within limits. Crucial time is saved by feeding the patient's readings against such parameters into supervised machine learning models that can classify the survivability of a patient post surgery fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-aquatic",
   "metadata": {},
   "source": [
    "Question 3. Give three supervised learning examples.\n",
    "\n",
    "Answer 3)\n",
    "\n",
    " 1. Sentiment Analysis on a product's reviews to determine how well the product is doing on an e-commerce platform.\n",
    " \n",
    " 2. Spam detection to remove e-mails that are advertising based.\n",
    " \n",
    " 3. Prediction of salary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-throat",
   "metadata": {},
   "source": [
    "Question 4. In supervised learning, what are classification and regression?\n",
    "\n",
    "Answer 4) In supervised learning, classification and regression are two fundamental types of tasks, and they involve different types of predictions based on the input data and the desired output. Here's a brief explanation of each:\n",
    "\n",
    "Classification:\n",
    "\n",
    "Objective: The main goal of classification is to assign a category or label to a given input data point. It is used when the output is categorical or discrete.\n",
    "Examples: Image recognition (e.g., classifying images of animals into \"cats,\" \"dogs,\" \"birds,\" etc.), spam email detection (classifying emails as \"spam\" or \"not spam\"), sentiment analysis (classifying text as \"positive,\" \"negative,\" or \"neutral\").\n",
    "Output: The output of a classification model is a discrete category or label, and the model's job is to correctly classify input data points into one of these categories.\n",
    "Regression:\n",
    "\n",
    "Objective: Regression is used when the goal is to predict a continuous numeric value or quantity. It is used when the output is numerical and can take on a wide range of values.\n",
    "Examples: Predicting house prices (given features like square footage, number of bedrooms, etc.), forecasting stock prices, estimating a person's age based on certain characteristics.\n",
    "Output: The output of a regression model is a numeric value, and the model's task is to provide the best estimate or prediction of this value based on the input data.\n",
    "Here's a simple analogy to distinguish between classification and regression:\n",
    "\n",
    "Classification is like sorting objects into different labeled bins. You're deciding which category each object belongs to, like sorting fruits into \"apples,\" \"bananas,\" and \"oranges.\"\n",
    "\n",
    "Regression is like estimating a numerical value. You're predicting a specific quantity, like estimating the weight of a fruit in grams or the price of a house in dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-italic",
   "metadata": {},
   "source": [
    "Question 5. Give some popular classification algorithms as examples.\n",
    "\n",
    " 1. Naive Bayes classifier\n",
    " \n",
    " 2. Random Forest classifier\n",
    " \n",
    " 3. Support Vector classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-apparel",
   "metadata": {},
   "source": [
    "Question 6. Briefly describe the SVM model.\n",
    "\n",
    "SVM or Support Vector Machine model is a machine learning model that can be used for classification and regression model.\n",
    "Imagine you have a bunch of dots on a piece of paper, and these dots belong to two different groups. Your job is to draw a line on the paper in such a way that it separates these two groups of dots as clearly as possible. This line you draw is your \"decision boundary.\"\n",
    "\n",
    "Now, here's where SVM comes in:\n",
    "\n",
    "Margin: SVM wants to find the best line (decision boundary) that not only separates the dots but also has the biggest gap (or margin) between the two groups. The margin is like the empty space between the line and the nearest dots from both groups.\n",
    "\n",
    "Support Vectors: These are the dots that are closest to the decision boundary. They are like the \"supporting actors\" in the SVM movie. SVM pays special attention to them because they help define the margin.\n",
    "\n",
    "Maximizing the Margin: The main goal of SVM is to find the line (or decision boundary) that maximizes this gap or margin while making sure that it doesn't touch or cross over any dots. It's like finding the \"safest\" path between the two groups.\n",
    "\n",
    "Kernel Trick: Sometimes, the dots are not just in a simple pattern that can be separated with a straight line. In such cases, SVM can use something called a \"kernel trick\" to transform the problem into a higher-dimensional space where a straight line can do the job.\n",
    "\n",
    "In a nutshell, SVM is like a smart ruler that tries to draw a line in a way that keeps different groups of dots apart as much as possible, using the support vectors as guides, and it can even bend and twist in a higher-dimensional world if needed. It's great for tasks like classifying things into different categories when you have clear groups in your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-wisconsin",
   "metadata": {},
   "source": [
    "Question 7. In SVM, what is the cost of misclassification?\n",
    "\n",
    "Answer 7) \n",
    "\n",
    "\n",
    "In Support Vector Machines (SVM), the cost of misclassification is a parameter that you can control to determine how much you penalize errors in your model. It's also known as the \"cost parameter\" or \"C.\"\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "Low Cost (Small C): If you set a low cost, the SVM model is more tolerant of misclassifications in the training data. In other words, it allows some data points to be on the wrong side of the decision boundary. This can lead to a larger margin but may result in some training data points being misclassified.\n",
    "\n",
    "High Cost (Large C): If you set a high cost, the SVM model becomes less tolerant of misclassifications. It tries harder to correctly classify all training data points, even if it means having a smaller margin or a more complex decision boundary. This can lead to a smaller margin but fewer training data points are misclassified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-latex",
   "metadata": {},
   "source": [
    "Question 8. In the SVM model, define Support Vectors.\n",
    "\n",
    "\n",
    "Answer 8)\n",
    "\n",
    "Support vectors in an SVM model are the data points that are closest to the decision boundary, also known as the hyperplane. The decision boundary is a line or plane in high-dimensional space that separates the data points into two classes.\n",
    "\n",
    "The support vectors play a vital role in defining the decision boundary and the margin of separation. The margin is the distance between the hyperplane and the closest data points. A larger margin means that the SVM model is more confident in its predictions.\n",
    "\n",
    "The SVM algorithm aims to find a hyperplane that maximizes the margin between the two classes of data points. The support vectors are the data points that determine the position and orientation of the hyperplane.\n",
    "\n",
    "Deleting a support vector from the training data would change the position of the hyperplane, which would affect the model's predictions. Therefore, the support vectors are very important for the performance of the SVM model.\n",
    "\n",
    "Here is an example of support vectors in a two-dimensional space:\n",
    "\n",
    "[0.5, 0.5]\n",
    "[-0.5, -0.5]\n",
    "[1.0, 1.0]\n",
    "[-1.0, -1.0]\n",
    "The decision boundary in this case is a line that passes through the middle of the two classes. The support vectors are the two data points that are closest to the decision boundary.\n",
    "\n",
    "SVM models are often used for classification tasks because they can be very accurate and robust to noise in the data. Support vectors are also useful for feature selection, as they identify the most important features for distinguishing between the two classes of data points.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-audio",
   "metadata": {},
   "source": [
    "Question 9. In the SVM model, define the kernel.\n",
    "\n",
    "Answer 9)\n",
    "\n",
    "In the SVM (Support Vector Machine) model, a kernel is a mathematical function that allows the algorithm to implicitly transform the input data into a higher-dimensional space. Kernels are used to make the SVM capable of finding complex, nonlinear decision boundaries between different classes of data.\n",
    "\n",
    "Here's a simple way to understand kernels:\n",
    "\n",
    "Linear Kernel: When you have data that can be separated by a straight line or plane (in the case of two or three dimensions), a linear kernel is used. It essentially computes the dot product between the input data points in the original feature space.\n",
    "\n",
    "Nonlinear Kernels: In many real-world scenarios, data is not separable by a simple straight line. Nonlinear kernels, such as polynomial kernels and radial basis function (RBF) kernels, transform the input data into a higher-dimensional space where it becomes easier to separate the data using a linear decision boundary.\n",
    "\n",
    "Polynomial Kernel: It maps the data into a higher-dimensional space using polynomial functions. For example, a polynomial kernel of degree 2 will transform the data into a space where quadratic decision boundaries are more appropriate.\n",
    "\n",
    "RBF (Radial Basis Function) Kernel: The RBF kernel uses a Gaussian-like function to map the data into an infinite-dimensional space. This allows SVM to create highly flexible decision boundaries that can adapt to complex data distributions.\n",
    "\n",
    "In essence, kernels enable SVM to handle data that is not linearly separable by projecting it into a higher-dimensional space, where it might become linearly separable. The choice of the kernel and its parameters is a critical aspect of training an SVM model, and it depends on the nature of the data and the problem you are trying to solve. Different kernels may be more suitable for different types of datasets, and selecting the right one can significantly impact the performance of the SVM.\n",
    "\n",
    "In the SVM model, different mathematical functions can be used in place of the (transpose(xi)*x) of the dual problem for various implicit feature transformations. Such mathematical functions that can make SVM work with nearly any form of non linear classification and regression is called the kernel. For example, String kernels can be used to make SVM usable for NLP applications.\n",
    "\n",
    "f(x)=NXiÎ±iyi(xiTx)+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-alarm",
   "metadata": {},
   "source": [
    "Question 10. What are the factors that influence SVM's effectiveness?\n",
    "\n",
    "Answer 10)\n",
    "\n",
    "The effectiveness of a Support Vector Machine (SVM) model depends on various factors and considerations. Here are some key factors that can influence the performance of an SVM:\n",
    "\n",
    "Choice of Kernel: Selecting the appropriate kernel function is crucial. The choice of kernel depends on the nature of the data and whether it can be effectively separated in a higher-dimensional space.\n",
    "\n",
    "Kernel Parameters: Kernel functions often have associated parameters (e.g., degree for polynomial kernels, gamma for RBF kernels). Tuning these parameters correctly is essential for achieving the best results. G\n",
    "\n",
    "Regularization Parameter (C): The regularization parameter C controls the trade-off between maximizing the margin and minimizing classification errors on the training data. Choosing the right value for C is important. A smaller C leads to a larger margin but may tolerate some misclassifications, while a larger C aims to classify training data correctly but may result in a smaller margin.\n",
    "\n",
    "Data Quality: The quality and cleanliness of the training data significantly impact SVM performance. Outliers, noise, and missing data can affect the model's ability to learn an accurate decision boundary.\n",
    "\n",
    "Feature Engineering: Proper feature selection and engineering can enhance SVM performance..\n",
    "\n",
    "Class Imbalance: In cases where one class has significantly more data points than another (class imbalance), SVM's performance may be skewed. \n",
    "Cross-Validation: Properly assessing model performance through techniques like cross-validation helps in avoiding overfitting and ensuring that the model generalizes well to unseen data.\n",
    "\n",
    "Data Scaling: SVM is sensitive to the scale of input features. Scaling or normalizing features can ensure that no single feature dominates the learning process.\n",
    "\n",
    "Dimensionality: SVM can perform well with high-dimensional data, but it may become less interpretable. Techniques like feature selection or dimensionality reduction (e.g., PCA) can be beneficial.\n",
    "\n",
    "Hyperparameter Tuning: Fine-tuning hyperparameters and using techniques like grid search or random search can help optimize the SVM model for a specific problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-nothing",
   "metadata": {},
   "source": [
    "Question 11. What are the benefits of using the SVM model?\n",
    "\n",
    "Answer 11)\n",
    "\n",
    "Effective in High-Dimensional Spaces: SVMs perform well even in high-dimensional spaces, which is especially valuable in fields like text classification, image recognition, and genomics, where data often has many features.\n",
    "\n",
    "Robust Against Overfitting: SVMs have a regularization parameter (C) that allows you to control the trade-off between maximizing the margin and minimizing classification errors. T\n",
    "\n",
    "Versatility: SVMs can be applied to both classification and regression tasks. They can handle both binary and multiclass classification problems.\n",
    "\n",
    "Effective in Nonlinear Cases: SVMs can model complex, nonlinear relationships between data points through the use of various kernel functions (e.g., polynomial, RBF). This flexibility makes them suitable for a wide range of data distributions..\n",
    "\n",
    "Outlier Resistance: SVMs are relatively less sensitive to outliers in the training data compared to some other machine learning algorithms. They focus on the support vectors, which are the most critical data points for defining the decision boundary.\n",
    "\n",
    "Memory Efficiency: SVMs require relatively less memory compared to some other complex models, making them suitable for scenarios with limited computational resources.\n",
    "\n",
    "Interpretability: In linear SVMs, the decision boundary is a linear combination of input features, making the model interpretable and allowing for feature importance analysis.\n",
    "\n",
    "Theoretical Foundation: SVMs are well-grounded in mathematics and have a strong theoretical foundation, which aids in understanding their behavior and performance.\n",
    "\n",
    "Wide Application: SVMs have been successfully applied in various domains, including image recognition, text classification, bioinformatics, finance, and more. Their versatility makes them applicable to a broad range of problems.\n",
    "\n",
    "Support for Unstructured Data: SVMs can handle various types of data, including structured data (tabular data) and unstructured data (text, images, audio) when used with appropriate feature engineering.\n",
    "\n",
    "Few Hyperparameters: SVMs have relatively few hyperparameters to tune, making them less prone to overfitting during hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-gateway",
   "metadata": {},
   "source": [
    "Question 12. What are the drawbacks of using the SVM model?\n",
    "\n",
    "Answer 12)\n",
    "\n",
    "A. Finding or coming up with problem specific kernels for best performance of SVM is difficult.\n",
    " \n",
    " B. No inherent way of getting feature importance.\n",
    " \n",
    " C. Built model has low interpretability\n",
    " \n",
    " D. Presence of two hyperparameters makes tuning time large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-founder",
   "metadata": {},
   "source": [
    "Question 13. Notes should be written on\n",
    "\n",
    "a) The kNN algorithm has a validation flaw.\n",
    "\n",
    "The k-Nearest Neighbors (kNN) algorithm is a simple yet effective machine learning algorithm for classification and regression tasks. However, it does have a validation flaw, primarily related to its sensitivity to the choice of the hyperparameter \"k.\"\n",
    "\n",
    "The validation flaw in kNN arises from the fact that it often requires selecting the optimal value of \"k,\" which represents the number of nearest neighbors to consider when making predictions. The selection of \"k\" can significantly impact the algorithm's performance, and finding the right \"k\" value is not always straightforward.\n",
    "\n",
    "The validation flaw can manifest in the following ways:\n",
    "\n",
    "Overfitting and Underfitting: If \"k\" is too small (e.g., 1 or 2), the kNN algorithm may become overly sensitive to noise in the training data, leading to overfitting. Conversely, if \"k\" is too large, it may oversmooth the decision boundary and result in underfitting.\n",
    "\n",
    "Validation Strategy: Determining the best \"k\" often involves using techniques like cross-validation, which can be computationally expensive and time-consuming, especially for large datasets or high-dimensional spaces.\n",
    "\n",
    "Data Characteristics: The optimal \"k\" value may vary depending on the specific dataset and problem. What works well for one dataset may not work for another.\n",
    "\n",
    "Curse of Dimensionality: In high-dimensional spaces, the effectiveness of kNN can degrade because distance metrics become less informative. This can make the selection of \"k\" even more challenging.\n",
    "\n",
    "\n",
    "b) In the kNN algorithm, the k value is chosen.\n",
    "\n",
    "In the k-Nearest Neighbors (kNN) algorithm, the value of \"k\" represents the number of nearest neighbors to consider when making predictions. Choosing an appropriate \"k\" value is a critical decision in the kNN algorithm, and it can significantly impact the algorithm's performance. Here's how the choice of \"k\" is typically made:\n",
    "\n",
    "Domain Knowledge: One approach to selecting the \"k\" value is to rely on domain knowledge or prior experience with the problem. If you have a good understanding of the data and the problem you are trying to solve, you may have insights into an appropriate range for \"k.\"\n",
    "\n",
    "Exploratory Data Analysis: Analyzing the dataset and visualizing the data can provide clues about the nature of the relationships between data points. You can look for patterns, clusters, or variations in the data that may inform the choice of \"k.\"\n",
    "\n",
    "Cross-Validation: Cross-validation is a common technique for selecting the best \"k\" value. It involves splitting the dataset into training and validation sets multiple times, each time using a different \"k\" value. The \"k\" that results in the best performance (e.g., lowest error rate) on the validation set is chosen as the optimal \"k.\"\n",
    "\n",
    "Grid Search: Grid search is a systematic approach that involves evaluating the algorithm's performance for a range of \"k\" values. You specify a set of \"k\" values to consider, and the algorithm is trained and validated for each \"k\" value. The \"k\" value that yields the best performance is selected.\n",
    "\n",
    "Testing Multiple Values: It's common to test a range of \"k\" values, starting from small values (e.g., 1 or 3) and gradually increasing to larger values (e.g., 5, 7, 9, etc.). This allows you to observe how the model's performance changes with \"k\" and choose the point where performance stabilizes or improves.\n",
    "\n",
    "Trade-Off: Keep in mind that smaller \"k\" values make the model more sensitive to noise and outliers in the data, potentially leading to overfitting. Larger \"k\" values may oversmooth the decision boundary, resulting in underfitting. The choice of \"k\" represents a trade-off between bias and variance.\n",
    "\n",
    "c). A decision tree with inductive bias\n",
    "\n",
    "The inductive bias in decision tree is that shorter trees are preferred over longer trees. Smaller trees would mean smaller depths with small number of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-google",
   "metadata": {},
   "source": [
    "Question 14. What are some of the benefits of the kNN algorithm? \n",
    "\n",
    "Answer 14)\n",
    "\n",
    "here are some of the benefits of the k-Nearest Neighbors (kNN) algorithm:\n",
    "\n",
    "Simplicity: kNN is easy to understand and implement. It's a straightforward algorithm that doesn't require complex mathematical formulations or extensive hyperparameter tuning.\n",
    "\n",
    "No Assumptions: kNN makes no strong assumptions about the underlying data distribution, making it applicable to a wide range of problem types and data scenarios.\n",
    "\n",
    "Non-Parametric: kNN is a non-parametric algorithm, meaning it doesn't make specific assumptions about the functional form of the decision boundary. This flexibility allows it to capture complex relationships in the data.\n",
    "\n",
    "Effective for Multimodal Data: kNN can handle datasets with multiple classes or clusters effectively. It can adapt to the local structure of the data, making it suitable for problems with complex boundaries.\n",
    "\n",
    "Lazy Learning: kNN is a lazy learner, meaning it doesn't build an explicit model during training. Instead, it memorizes the training data, which can be advantageous when dealing with evolving or streaming data, as it can adapt quickly to changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-payroll",
   "metadata": {},
   "source": [
    "Question 15. What are some of the kNN algorithm&#39;s drawbacks?\n",
    "\n",
    "answer 15)\n",
    "\n",
    "A. Sensitive to outliers.\n",
    " \n",
    " B. Does not work well with high dimensionality.\n",
    " \n",
    " C. Sensitive to missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-party",
   "metadata": {},
   "source": [
    "Question 16. Explain the decision tree algorithm in a few words.\n",
    "\n",
    "Answer 16)\n",
    "\n",
    "Decision Tree algorithm is a supervised machine learning algorithm where an inverted tree is created and data is split into nodes based on a threshold or condition passed or failed.\n",
    "Think of a decision tree as a flowchart for making decisions. It's like a tree where each branch represents a choice or decision, and each leaf node represents an outcome or result.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "Starting Point: At the top of the tree, you have a question or condition based on some feature of your data. This question helps you decide which way to go.\n",
    "\n",
    "Branches: Depending on the answer to the question, you follow one of the branches, which leads to another question or condition. Each branch represents a different choice or action.\n",
    "\n",
    "Leaves: Eventually, you reach the end of a branch, which is a leaf node. At the leaves, you find the final decision or prediction. It's like saying, \"If you follow this path of questions and answers, here's what you should do or expect.\"\n",
    "\n",
    "Splitting: The tree keeps branching and splitting based on the features of your data until it reaches a stopping point. You decide when to stop based on criteria like accuracy or simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-advice",
   "metadata": {},
   "source": [
    "Question 17. What is the difference between a node and a leaf in a decision tree?\n",
    "\n",
    "Answer 17)\n",
    "\n",
    "Node in a decision tree is a collection of data points for which decision has to be made whether to split or not, and how to split using a mathematical function.\n",
    "\n",
    "Leaf in a decision tree is a terminating node or a terminating vector, beyond which no splits will be done.\n",
    "\n",
    "Node:\n",
    "\n",
    "Internal Decision Points: Nodes are the internal points in a decision tree where decisions or questions are made based on the features or attributes of the data.\n",
    "\n",
    "Splitting Criteria: At each node, the decision tree algorithm evaluates a specific feature and its value to decide how to split the data into subsets. These splits create branches that lead to other nodes or leaf nodes.\n",
    "\n",
    "Intermediate Decisions: Nodes represent intermediate decisions or conditions in the decision-making process. For example, a node might ask, \"Is the temperature greater than 30 degrees Celsius?\"\n",
    "\n",
    "Further Branching: Nodes direct the flow of the decision tree, determining which branch to follow based on the answer to the question they pose.\n",
    "\n",
    "Leaf (or Terminal Node):\n",
    "\n",
    "Final Decisions or Predictions: Leaves are the endpoints of branches in a decision tree. They do not pose any further questions or conditions.\n",
    "\n",
    "Outcome or Prediction: Each leaf node represents a final decision or prediction. For example, in a classification task, a leaf might indicate the class label to assign to an input data point.\n",
    "\n",
    "No Further Splitting: Unlike nodes, leaves do not split the data any further. They are the stopping points in the decision tree.\n",
    "\n",
    "Result or Action: Leaf nodes provide the result or action to take based on the path of decisions made by following the branches from the root node to that specific leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-starter",
   "metadata": {},
   "source": [
    "Question 18. What is a decision tree's entropy?\n",
    "\n",
    "Answer 18)\n",
    "\n",
    "In the context of a decision tree, entropy is a concept used to measure the impurity or disorder of a set of data points within a particular node. It's often employed as a criterion for deciding how to split data at each node when constructing the decision tree. Lower entropy indicates purer subsets of data, while higher entropy suggests more mixed or impure subsets.\n",
    "\n",
    "Here's a simple explanation of entropy in decision trees:\n",
    "\n",
    "Entropy as a Measure of Disorder: Think of a node in a decision tree as a collection of data points, and imagine that these data points belong to different classes or categories (e.g., \"yes\" or \"no,\" \"red\" or \"blue\"). The entropy of the node measures how mixed or disordered these classes are within that node.\n",
    "\n",
    "Entropy(S) = -p1 * log2(p1) - p2 * log2(p2) - ... - pn * log2(pn)\n",
    "\n",
    "In this formula, \"Entropy(S)\" represents the entropy of a node \"S,\" and \"p1, p2, ..., pn\" are the proportions of data points belonging to each class within that node. The logarithm is usually base 2.\n",
    "\n",
    "Interpretation: If the data in a node is perfectly pure (i.e., all data points belong to the same class), then the entropy is 0. On the other hand, if the data is evenly split among different classes, the entropy is at its maximum (e.g., log2(2) = 1 for binary classification).\n",
    "\n",
    "Decision Tree Splitting: When constructing a decision tree, you want to minimize entropy at each node. To do this, you consider different ways to split the data based on features or attributes. The goal is to find the split that minimizes the weighted average of entropy in the resulting child nodes. This helps create a decision tree that separates data into increasingly pure subsets as you move down the tree.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-hampton",
   "metadata": {},
   "source": [
    "Question 19. In a decision tree, define knowledge gain.\n",
    "\n",
    "\n",
    "Answer 19 )\n",
    "\n",
    "Numerical value used to quantify the quality of a split is called knowledge gain or information gain. It is calculated by subtracting the weighted entropies for each branch from the original entropy. Maximising information gain leads to best results in decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-moment",
   "metadata": {},
   "source": [
    "Question 20. Choose three advantages of the decision tree approach and write them down.\n",
    "\n",
    "Answer 20)\n",
    "\n",
    "A. A function in the form of gini index is used and solved for getting mathematically sound splits and end classification.\n",
    " \n",
    " B. Decision Trees can be visually realized, and have high interpretability.\n",
    " \n",
    " C. Decision Tree approach is not distance based, only based on order and logic. Due to this, many preprocessing methods need not be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-final",
   "metadata": {},
   "source": [
    "Question 21. Make a list of three flaws in the decision tree process.\n",
    "\n",
    "Answer 21)\n",
    "A. Decision Trees are inherently overfitting.\n",
    " \n",
    " B. They do not perform well for high dimensional data.\n",
    " \n",
    " C. Training an accurate model for continous data is difficult as little changes in the training data would lead to different results in classification on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-little",
   "metadata": {},
   "source": [
    "Question 22. Briefly describe the random forest model.\n",
    "\n",
    "Answer 22)\n",
    "\n",
    "Random forest model is an aggregation ensemble model technique based on decision trees. Multiple base learner decision trees are trained on subsets of data and each comes up with its own classification result.\n",
    "The Random Forest model is an ensemble learning technique in machine learning. It's designed to improve the performance and robustness of decision trees, which can sometimes be prone to overfitting and high variance. Here's a brief description of the Random Forest model:\n",
    "\n",
    "Ensemble of Decision Trees: A Random Forest consists of a collection (ensemble) of decision trees. These decision trees are constructed independently from each other.\n",
    "\n",
    "Randomness in Tree Building: The \"random\" in Random Forest comes from two sources of randomness:\n",
    "\n",
    "Random Sampling: When building each tree, the algorithm randomly selects a subset of the training data (with replacement). This is called bootstrapping or bagging. It ensures that each tree sees a slightly different version of the data.\n",
    "Random Feature Selection: At each node of a decision tree, instead of considering all features for splitting, the algorithm randomly selects a subset of features to be considered. This helps in reducing the correlation between trees.\n",
    "Voting or Averaging: Once all the individual decision trees are built, they are used for predictions. In classification tasks, each tree \"votes\" for a class, and in regression tasks, they provide predictions. The final prediction is typically the majority vote (for classification) or the average (for regression) of the predictions from all the trees.\n",
    "\n",
    "Reducing Overfitting: By introducing randomness in the construction of each decision tree and combining their results, Random Forest reduces overfitting. It improves the model's ability to generalize to new, unseen data.\n",
    "\n",
    "Highly Robust and Accurate: Random Forests are known for their robustness and accuracy. They are less sensitive to outliers and noisy data, and they tend to perform well across a wide range of datasets and problem types.\n",
    "\n",
    "Feature Importance: Random Forests can provide a measure of feature importance, indicating which features are most influential in making predictions. This is valuable for feature selection and understanding the importance of different factors in your data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
