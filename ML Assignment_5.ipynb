{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aab3415a",
   "metadata": {},
   "source": [
    "# Assignment_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14da2db",
   "metadata": {},
   "source": [
    "Question 1. What are the key tasks that machine learning entails? What does data pre-processing imply?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbdfb78",
   "metadata": {},
   "source": [
    "Key tasks under machine learning are \n",
    " 1. Defining an objective\n",
    " 2. Collection of data\n",
    " 3. Data preprocessing\n",
    " 4. Choose the model\n",
    " 5. Train model\n",
    " 6. Evaluate using performance\n",
    " 7. Tune the hyperparameters for best performance\n",
    " 8. Produce results\n",
    " \n",
    "Data Pre-processing includes steps to make data ready to be used for machine learning modeling. It includes, but is not limited to data cleaning, standardizing of keywords or headers, taking care of missing values and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c08594",
   "metadata": {},
   "source": [
    "Question 2. Describe quantitative and qualitative data in depth. Make a distinction between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57281729",
   "metadata": {},
   "source": [
    "Quantitative data is one that is numerical in nature and has order and mathematical meaning to it. It can be of two types\n",
    "\n",
    " 1. Discrete quantitative data\n",
    " This type of data has values that come from a countably finite or uncountably finite set. For example: number of planets in Milky Way galaxy. It is generally visualized using bar plots.\n",
    " \n",
    " 2. Continous quantitative data\n",
    " This type of data has values that come from uncountably infinite sets. For example: heights of Indians. It is generally visualized using histograms.\n",
    " \n",
    "Qualitative data is one that can be numerical or string in nature, does not necessarily have order to it but cannot possess mathematical meaning. For example: Blood groups, Grades etc.\n",
    "\n",
    " 1. Nominal qualitative data\n",
    " This type of data is mostly used to label data points based on other characteristics. Nominal data cannot be ordered in any way. For example: Classification of flowers into sub species based on characteristics like sepal length etc.\n",
    " \n",
    " 2. Ordinal qualitative data\n",
    " This type of data is one that can be ordered. For example: Grades.\n",
    " \n",
    "Quantitative data is different from qualitative data as it does not have any numerical nature and do not have mathematical meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6315a75b",
   "metadata": {},
   "source": [
    "Question 3. Create a basic data collection that includes some sample records. Have at least one attribute from\n",
    "each of the machine learning data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6e57238",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\"names\":{\"Akash, Deep, Sahoo\"},\"marks\":{87,78,70},\"date\":{\"November,2021\",\"November,2021\"},\"grade\":{\"A\",\"B\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b86ed502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'names': {'Akash, Deep, Sahoo'},\n",
       " 'marks': {70, 78, 87},\n",
       " 'date': {'November,2021'},\n",
       " 'grade': {'A', 'B'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8fcd9c",
   "metadata": {},
   "source": [
    "Question 4. What are the various causes of machine learning data issues? What are the ramifications?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d7a63d",
   "metadata": {},
   "source": [
    "Answer 4 )\n",
    "Machine learning data issues can significantly impact the performance and reliability of machine learning models. These issues can arise from various causes, and they can have significant ramifications. Here are some common causes of machine learning data issues and their potential consequences:\n",
    "\n",
    "1. Data Quality Issues:\n",
    "\n",
    "Inaccurate Data: Data may contain errors, inaccuracies, or inconsistencies due to human errors during data collection or data entry.\n",
    "Missing Data: Some data points may be missing due to various reasons, such as non-responses or technical issues during data collection.\n",
    "Ramifications: Inaccurate or missing data can lead to biased and unreliable model predictions. Models trained on such data may not generalize well to new data, and decision-making based on these models may be flawed.\n",
    "\n",
    "2. Imbalanced Data:\n",
    "\n",
    "Class Imbalance: In classification problems, one class may significantly outnumber the others, leading to an imbalanced dataset.\n",
    "Ramifications: Imbalanced data can cause machine learning models to be biased towards the majority class. They may perform poorly in predicting the minority class, which can be critical in some applications.\n",
    "\n",
    "3. Noisy Data:\n",
    "\n",
    "Noise in Labels: Labels or target values assigned to data points may contain errors or inconsistencies.\n",
    "Ramifications: Noisy labels can mislead the model during training, reducing its accuracy and reliability.\n",
    "\n",
    "4. Feature Engineering Issues:\n",
    "\n",
    "Irrelevant Features: Some features may not be useful for making predictions and can add noise to the data.\n",
    "Collinearity: High correlation between features can lead to multicollinearity issues.\n",
    "Ramifications: Irrelevant features can increase model complexity and slow down training. Collinearity can make it difficult to identify the true relationship between features and the target variable.\n",
    "\n",
    "5. Data Sampling Bias:\n",
    "\n",
    "Selection Bias: Data collection processes may introduce biases, such as only collecting data from certain groups or underrepresenting specific populations.\n",
    "Ramifications: Sampling bias can lead to models that are not representative of the entire population, making predictions skewed or unfair.\n",
    "\n",
    "6. Data Leakage:\n",
    "\n",
    "Information Leakage: Data may inadvertently contain information about the target variable or future events that should not be available during model training.\n",
    "Ramifications: Data leakage can lead to over-optimistic model performance during training but result in poor generalization to new, unseen data.\n",
    "\n",
    "7. Outliers:\n",
    "\n",
    "Extreme Values: Outliers, which are extreme data points, can skew the distribution and affect model performance.\n",
    "Ramifications: Outliers can lead to inaccurate model predictions, especially in models that are sensitive to extreme values.\n",
    "\n",
    "8. Data Scaling Issues:\n",
    "\n",
    "Feature Scaling: Features with different scales may affect the training process of some algorithms.\n",
    "Ramifications: Poorly scaled data can cause certain machine learning algorithms to converge slowly or perform suboptimally.\n",
    "\n",
    "9. Labeling Bias:\n",
    "\n",
    "Human Bias: Human annotators may introduce biases when labeling data, leading to biased models.\n",
    "Ramifications: Biased models can make unfair decisions or perpetuate existing biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55f5fed",
   "metadata": {},
   "source": [
    "Question 5. Demonstrate various approaches to categorical data exploration with appropriate examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49940c8e",
   "metadata": {},
   "source": [
    "Answer 5) \n",
    "Exploring categorical data is essential to understand the distribution, relationships, and patterns within the categorical variables in a dataset. Here are various approaches to explore categorical data with appropriate examples:\n",
    "\n",
    "1. Frequency Distribution:\n",
    "\n",
    "Display the frequency (count) of each category within a categorical variable.\n",
    "Example: Suppose you have a dataset of customer reviews with a \"Rating\" column, where ratings can be \"Excellent,\" \"Good,\" \"Average,\" or \"Poor.\" You can create a frequency distribution table or bar chart to see how many reviews fall into each rating category.\n",
    "\n",
    "2. Bar Charts:\n",
    "\n",
    "Visualize categorical data using bar charts to compare the counts or percentages of each category.\n",
    "Example: In a survey dataset, you can create a bar chart to show the distribution of respondents' favorite colors (e.g., red, blue, green) and see which color is the most popular among the participants.\n",
    "\n",
    "3. Pie Charts:\n",
    "\n",
    "Use pie charts to visualize the proportions of different categories within a categorical variable as segments of a circle.\n",
    "Example: You can use a pie chart to display the market share of various smartphone brands (e.g., Apple, Samsung, Google) in a mobile phone market analysis.\n",
    "\n",
    "4. Stacked Bar Charts:\n",
    "\n",
    "Show the relationship between two categorical variables using stacked bar charts. Each bar is divided into segments representing different categories of the second variable.\n",
    "Example: In a survey about preferred music genres, you can create stacked bar charts to visualize how genre preferences vary among different age groups (e.g., rock music preference among teenagers vs. adults).\n",
    "\n",
    "5. Cross-Tabulation (Contingency Tables):\n",
    "\n",
    "Create cross-tabulation tables to explore the relationship between two categorical variables. It shows how often combinations of categories occur.\n",
    "Example: In a dataset about customer purchasing behavior, you can create a cross-tabulation table to see how the \"Payment Method\" and \"Product Category\" variables are related, indicating which payment methods are commonly used for specific types of products.\n",
    "\n",
    "6. Mosaic Plots:\n",
    "\n",
    "Mosaic plots are used to visualize the relationship between two categorical variables, showing the proportions of different categories in each cell.\n",
    "Example: In a survey dataset, you can create a mosaic plot to explore the association between \"Education Level\" and \"Preferred Vacation Destinations,\" revealing whether certain destinations are more popular among people with specific education levels.\n",
    "\n",
    "7. Heatmaps:\n",
    "\n",
    "Heatmaps can display the frequency or proportion of combinations of two categorical variables using colors. Darker colors represent higher frequencies.\n",
    "Example: In a dataset about movie preferences, you can create a heatmap to visualize which movie genres are most frequently preferred by different age groups and genders.\n",
    "\n",
    "8. Word Clouds:\n",
    "\n",
    "For text data with categorical information (e.g., customer reviews with sentiment labels), word clouds can visualize the most frequent terms associated with each category.\n",
    "Example: When analyzing product reviews, you can generate word clouds to visualize the most common words used in \"Positive\" and \"Negative\" sentiment reviews separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f6a885",
   "metadata": {},
   "source": [
    "Question 6. How would the learning activity be affected if certain variables have missing values? Having said\n",
    "that, what can be done about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6d80cd",
   "metadata": {},
   "source": [
    "Answer 6)\n",
    "Missing values can have a significant impact on the learning activity of a machine learning model. If a model is trained on data with missing values, it will not be able to learn the relationships between the variables as accurately as if the data was complete. This can lead to a number of problems, such as:\n",
    "\n",
    "Reduced accuracy: The model may not be able to predict the target variable as accurately as if the data was complete.\n",
    "\n",
    "Increased bias: The model may be biased towards certain categories of data if the missing values are not distributed randomly.\n",
    "\n",
    "Overfitting: The model may overfit the training data and not generalize well to new data.\n",
    "\n",
    "There are a number of things that can be done to mitigate the impact of missing values on the learning activity of a machine learning model:\n",
    "\n",
    "Remove the observations with missing values: This is the simplest approach, but it can lead to a loss of data.\n",
    "\n",
    "Impute the missing values: This involves estimating the missing values based on the other values in the dataset. There are a \n",
    "\n",
    "number of different imputation techniques available, such as mean imputation, median imputation, and mode imputation.\n",
    "\n",
    "Use algorithms that can handle missing values: Some machine learning algorithms are able to handle missing values without the need for imputation. These algorithms are often referred to as missing value robust algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda61bf3",
   "metadata": {},
   "source": [
    "Question 7. Describe the various methods for dealing with missing data values in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c490279",
   "metadata": {},
   "source": [
    "Answer 7)\n",
    "There are a number of methods for dealing with missing data values, including:\n",
    "\n",
    "Removing the observations with missing values: \n",
    "This is the simplest approach, but it can lead to a loss of data. \n",
    "\n",
    "Imputing the missing values: This involves estimating the missing values based on the other values in the dataset. There are a number of different imputation techniques available, such as:\n",
    "\n",
    "Mean imputation: This involves replacing the missing values with the mean value of the variable.\n",
    "\n",
    "Median imputation: This involves replacing the missing values with the median value of the variable.\n",
    "\n",
    "Mode imputation: This involves replacing the missing values with the most frequent value of the variable.\n",
    "\n",
    "K-nearest neighbors (KNN) imputation: This involves identifying the K most similar observations to the observation with the missing value and then using the values of those observations to impute the missing value.\n",
    "\n",
    "Multiple imputation: This involves creating multiple imputed datasets and then averaging the results of the model when it is trained on each imputed dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ce7e27",
   "metadata": {},
   "source": [
    "Question 8. What are the various data pre-processing techniques? Explain dimensionality reduction and\n",
    "function selection in a few words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eef12e",
   "metadata": {},
   "source": [
    "Answer 8)\n",
    "Data preprocessing involves a series of techniques and steps applied to raw data before it is used for analysis or modeling. Here are some common data preprocessing techniques, along with brief explanations of dimensionality reduction and feature selection:\n",
    "\n",
    "1. Data Cleaning:\n",
    "\n",
    "Removing or correcting errors, inaccuracies, and inconsistencies in the dataset, including handling missing values.\n",
    "\n",
    "2. Data Transformation:\n",
    "\n",
    "Converting data into a suitable format for analysis, such as normalizing or scaling numerical features.\n",
    "\n",
    "3. Data Imputation:\n",
    "\n",
    "Filling in missing values using methods like mean imputation, regression imputation, or multiple imputation.\n",
    "\n",
    "4. Encoding Categorical Variables:\n",
    "\n",
    "Representing categorical variables as numerical values, often through techniques like one-hot encoding or label encoding.\n",
    "\n",
    "5. Outlier Detection and Treatment:\n",
    "\n",
    "Identifying and handling outliers to prevent them from affecting the analysis or model.\n",
    "\n",
    "6. Feature Engineering:\n",
    "\n",
    "Creating new features or transforming existing ones to capture meaningful information for modeling.\n",
    "\n",
    "7. Data Scaling:\n",
    "\n",
    "Standardizing or normalizing numerical features to ensure they are on similar scales, preventing certain algorithms from being dominated by features with larger magnitudes.\n",
    "\n",
    "8. Data Splitting:\n",
    "- Splitting the dataset into training, validation, and test sets for model development, evaluation, and testing, respectively.\n",
    "\n",
    "9. Data Sampling:\n",
    "- Techniques like oversampling or undersampling to address class imbalance in classification problems.\n",
    "\n",
    "10. Handling Time-Series Data:\n",
    "- Resampling, interpolation, or feature extraction techniques tailored to time-series data.\n",
    "\n",
    "11. Data Integration:\n",
    "- Combining data from multiple sources or datasets to create a unified dataset for analysis.\n",
    "\n",
    "12. Handling Skewed Data:\n",
    "- Techniques like log transformation to address skewed data distributions.\n",
    "\n",
    "Dimensionality Reduction:\n",
    "\n",
    "Explanation: Dimensionality reduction techniques reduce the number of features (variables) in the dataset while preserving essential information.\n",
    "Methods: Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and more.\n",
    "Purpose: It helps reduce the complexity of the data, improve model efficiency, and mitigate the curse of dimensionality.\n",
    "\n",
    "Feature Selection:\n",
    "\n",
    "Explanation: Feature selection is the process of choosing the most relevant and informative features while discarding less important ones.\n",
    "Methods: Univariate feature selection, Recursive Feature Elimination (RFE), Feature Importance from tree-based models, and more.\n",
    "Purpose: It improves model interpretability, reduces overfitting, and reduces the time and resources needed for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baa6f5f",
   "metadata": {},
   "source": [
    "Question 9(i). What is the IQR? What criteria are used to assess it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afeed9e",
   "metadata": {},
   "source": [
    "\n",
    "IQR stands for Interquartile Range. It is a statistical measure that represents the spread or variability of a dataset, specifically the range covered by the middle 50% of the data. The IQR is calculated as the difference between the third quartile (Q3) and the first quartile (Q1) of a dataset.Here's how the IQR is calculated:\n",
    "\n",
    "First Quartile (Q1): This is the value below which 25% of the data falls. It is also the 25th percentile of the data.\n",
    "\n",
    "Third Quartile (Q3): This is the value below which 75% of the data falls. It is also the 75th percentile of the data.\n",
    "\n",
    "IQR = Q3 - Q1: The Interquartile Range is the difference between the third quartile and the first quartile.\n",
    "\n",
    "Criteria Used to Assess the IQR:\n",
    "\n",
    "Outlier Detection: The IQR is used to identify potential outliers in a dataset. Data points that fall below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR are considered outliers. \n",
    "Box Plots: The IQR is commonly represented in box plots, where the box represents the IQR, and the whiskers extend to the minimum and maximum values within a certain range (usually 1.5 times the IQR). Data points beyond the whiskers are marked as potential outliers.\n",
    "\n",
    "Data Distribution: The IQR helps in understanding the central 50% of the data's distribution. A narrow IQR indicates that most of the data points are clustered closely together, while a wide IQR suggests a more spread-out distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a7f49d",
   "metadata": {},
   "source": [
    "Question 9(ii) ii. Describe the various components of a box plot in detail? When will the lower whisker\n",
    "surpass the upper whisker in length? How can box plots be used to identify outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4bd9f8",
   "metadata": {},
   "source": [
    "A box plot, also known as a box and whisker plot, is a visual representation of the distribution of a numerical dataset. It is constructed by drawing a box around the middle 50% of the data, with a line down the middle to represent the median. Whiskers extend from the box to the minimum and maximum values, and outliers are plotted as individual points beyond the whiskers.\n",
    "\n",
    "The following are the different components of a box plot:\n",
    "\n",
    "Box: The box represents the middle 50% of the data, with the lower quartile (Q1) at the bottom and the upper quartile (Q3) at the top.\n",
    "\n",
    "Median: The median is the middle value in the dataset, and is represented by the line that divides the box in half.\n",
    "\n",
    "Whiskers: The whiskers extend from the box to the minimum and maximum values. The minimum value is the lowest value in the dataset, and the maximum value is the highest value in the dataset.\n",
    "\n",
    "Outliers: Outliers are data points that fall outside the whiskers. They are typically plotted as individual points beyond the whiskers.\n",
    "\n",
    "When will the lower whisker surpass the upper whisker in length?\n",
    "\n",
    "The lower whisker will surpass the upper whisker in length when the minimum value of the dataset is greater than the upper quartile. This can happen when the dataset is heavily skewed to the left.\n",
    "\n",
    "How can box plots be used to identify outliers?\n",
    "\n",
    "Outliers are data points that fall outside the whiskers of the box plot. This means that they are significantly different from the rest of the data. Box plots can be used to identify outliers by looking for data points that are plotted beyond the whiskers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76124d13",
   "metadata": {},
   "source": [
    "Question 10. Make brief notes on any two of the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76052730",
   "metadata": {},
   "source": [
    "i) Data collected at regular intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdfb798",
   "metadata": {},
   "source": [
    "Data collection can be in many different forms. One of the probabilistic sampling techniques used in Statistics is Stratified sampling in which data is collected at regular intervals but with a random start. It is used for its simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0c1f85",
   "metadata": {},
   "source": [
    "iii) Use a cross tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a48b4f",
   "metadata": {},
   "source": [
    "Cross tabs or cross tabulation are only used to find relations between two categorical variables where one categorical variable value would be giving row data and the other variable would give column data. Several tests like the chi square test can help find relations based on the cross tab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c1f4c",
   "metadata": {},
   "source": [
    "Question 11) Make a comparison between"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1a48a7",
   "metadata": {},
   "source": [
    "ii)Histogram and box plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923f0c41",
   "metadata": {},
   "source": [
    "Histograms are data visualizations used for continous quantitative data. Box Plots are data visualizations used for describing any quantitative data.\n",
    "\n",
    "Histograms are used for checking the distribution type. Box Plots are used for describing data in 5 point summary and detecting outliers in data.\n",
    "\n",
    "Granularity can be changed using bins in Histograms. Box Plots do no feature any control over granularity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb358a3",
   "metadata": {},
   "source": [
    "iii) Average and Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fc4d6d",
   "metadata": {},
   "source": [
    "Average is a measure of numerical centrality of a distribution. Median is a measure of positional centrality of a distribution\n",
    "\n",
    "Average can also be referred to as the value that is closest to most number of values in a data. Median can also be referred to as the value which is exactly in the middle of a distribution.\n",
    "\n",
    "Averages are affected by presence of outliers. Medians are not affected by outliers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
